# Meeting of TC39-TG4 - Main Track - 2023-10-11

## Folks

| Name       | GH Username | TLA | Affiliation |
| ---------- | ----------- | --- | ----------- |
| Jonathan   |             |     |             |
| Luca       |             |     |             |
| Bradford   |             |     |             |
| Ryan       |             |     |             |
| Jan        |             |     |             |
| Chengzhong |             |     |             |

## Agenda

- Munich recap
- Luca is working on a doc to submit to v8 folks about debug_ids
- Testing
  - Previous lots of discussion about validation but that’s a small part of it
  - Closure Compiler has a philosophy where every node in their original input AST must be pre
- Goal to have a standard by the next September plenary
  - Minimum would be test suite and correctness holes patched
  - Ideally would also have scopes feature and debug ID added
- Look through correctness

Notes:

- Ron Buckton brought up that, for generators, it would be great to know which offsets or points debuggers care about (related to [https://github.com/tc39/source-map-rfc/issues/38](https://github.com/tc39/source-map-rfc/issues/38))
  - Generators need the ability to be granular, for example emitting “cheap” source maps. But right now they have no guidelines into what a “full” source map should have or not have.
- At what granularity are source mappings made?
  - Ron: What granularity do debuggers care about?
  - Jon: Let’s enumerate the options, like line, token, etc.
  - Luca: Different tools care about different granularity. Sentry cares just about the line. A debugger might want the exact token inside a line. From a consuming tool point of view, different things are valid. Some tools generate code which does not map from the input.
  - Dan: It sounds like we could benefit from having multiple “profiles”. Maybe one would be per line and one would be per token.
  - Luca: Super granular source maps can be as valid as not-so-granular ones
- How do we test?
  - Brad: Make sure that all the input tokens have something that they correspond to in the output
  - If we go down the path of “profiles” we could have a series of tests that takes input files and source maps and validates that there are source map pointers for the appropriate (line, statement, token) locations.
- Types of tests
  - Generating source maps (validating, checking that input is there)
    - ??
    - Luca: Could write a short program that throws an error in a certain location. You run this program, you extract the stack trace, and then you give the test the location of where it was thrown. The test looks at your generated source map, tries to map the location back to the original source, and then you as a generating tool define where you map back to
      - The test doesn’t need to know your internal implementation details. You provide the code snippet, source map, original source.
      - Holger: This test isn’t self-contained, since the TS developers would come up with the original test in TS. So you can’t have a test that you can use in both and verify… but maybe then we should define a test template, so that everyone can define tests that are analogous. So it’d be a description of how someone can write this test, since we can’t cover it for all languages
      - Dan: This is specific to JS and to things which won’t remove throwing the exception, but that’s fine
      - Luca: Maybe you just need a position, not an exception?
      - Jon: Could run two programs and see that they do the same thing
    - Validator
      - Is it valid?
      - Does it correspond to stuff that exists in the source? Even if we just check that line and column exist (non JS languages might be harder to test here)
    - Specialized tests for JS-to-JS transforms? (only that subset)
      - Could check that you map to valid tokens
      - If we build up a suite of this, it could serve as a template for other tools
      - Brad’s idea of nodes that must be there because the tool would be changing the behavior if it didn’t map it. And you could map those around to confirm that it’s correct/present.
        - Luca: It’s hard to do that since some transforms will change behavior, so it’s unclear what you could count on.
    - Dan: It should be OK to have tests that only work against _some_ cases
  - Decoding stacks
    - Input: original code, generated code, source map, stack trace
    - Output: Decoded stack trace
    - Should be well-defined, just create a bunch of test cases
    - Luca: This is giving the line of code
    - Ryan: Should find out the right function names (hard)
      - Dan: Affected by scope information feature (we should have tests with and without the scope information)
  - Debuggers
    - Input: Original code, generated code, source map, line/column position (or, running program state?)
    - Output: Original line/column, local variable state if we have running program state,
  - Composition
    - Input: Two source maps
    - Output: One source map
    - Should be well-defined (needs us to define this algorithm)
- Brad: What are the goals of source maps? It only fully defines the offsets; what do we expect from names?
  - Dan: Names aren’t done perfectly yet, but it’s clearly a shared goal. People already do these other things with source maps, so we should develop definitions and tests even if the situation isn’t perfect.
  - Luca: Decoding stack traces works fine; it’s just inconsistencies in the tooling that generates it. Although some features don’t work yet like losing access to local bindings in the debugger. We would like to give you a test suite that can tell you whether you reach the quality bars, so that you can know which quality bar you meet.
- Luca: I plan to play around with these testing things
- Future meeting topic: Define profiles of mapping granularity, to make tooling more predictable and composable
